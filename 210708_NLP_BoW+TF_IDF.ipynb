{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210708_NLP_BoW+TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUvxXgbt8bsh17zGFjO4yH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhapis97/Practice_AI/blob/main/210708_NLP_BoW%2BTF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDPrOSXVo3tM"
      },
      "source": [
        "## Bag of Words(BoW)\n",
        "단어의 등장 순서를 고려하지 않는 **빈도수 기반**의 단어 표현 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg94B_nrpWXa"
      },
      "source": [
        "1. 각 단어에 고유한 정수 인덱스 부여\n",
        "2. 각 인덱스 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yucMo0aJp5jq"
      },
      "source": [
        "doc1 = 'John likes to watch movies. Mary likes movies too.'  \n",
        "Bow1 = {\"John\":1, 'likes':2, 'to':1, 'watch':1, 'movies':2, 'Mary':1, 'too':1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtszpusDqIg0",
        "outputId": "0fc198e5-f994-4a73-fbc4-f6e0d669b102"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/88/f817ef1af6f794e8f11313dcd1549de833f4599abcec82746ab5ed086686/JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 45.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfdYdq0ibht7"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "okt = Okt()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUeAqVryqEeZ",
        "outputId": "28bcd44b-dbe5-43b7-b15b-27bd9b41b6d0"
      },
      "source": [
        "# token = re.sub(\"(\\.)\", \"\", \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")\n",
        "token = re.sub(\"(\\.)\", \"\", \"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")\n",
        "# 정규표현식을 통해 온점을 제거하는 정제 작업이다.\n",
        "\n",
        "token = okt.morphs(token)\n",
        "# OKT형태소 분석기를 통해 토큰화 작업을 수행한 뒤에 token에다가 넣는다.\n",
        "\n",
        "word2index = {}\n",
        "bow = []\n",
        "for voca in token:\n",
        "  if voca not in word2index.keys():\n",
        "    word2index[voca] = len(word2index)\n",
        "    # token을 읽으면서, word2index에 없는(not in) 단어는 새로 추가하고, 이미 있는 단어는 넘긴다~\n",
        "    bow.insert(len(word2index)-1, 1)\n",
        "    # bow전체에 전부 기본값 1을 넣어준다. 단어의 개수는 최소 1개 이상이기 때문\n",
        "  else:\n",
        "    index = word2index.get(voca)\n",
        "    # 재등장하는 단어의 인덱스를 받아오기\n",
        "    bow[index] = bow[index]+1\n",
        "    # 재등장하는 단어는 해당하는 인덱스의 위치에 1을 더해줌(단어 개수 세는 것)\n",
        "    \n",
        "print(word2index)    # 단어의 인덱스 반환"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uECSVu1xrHqG",
        "outputId": "88e13a17-023e-442e-9d2f-5c5d9fcd00fe"
      },
      "source": [
        "bow    # bow 빈도수 반환"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdp6osMWsIO5"
      },
      "source": [
        "## Tensorflow의 Keras Tokenizer를 활용한 BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OXPCQ3ErWAi"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou9bdPmSseJo"
      },
      "source": [
        "def print_bow(sentence):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sentence)    # 단어장 생성\n",
        "  bow = dict(tokenizer.word_counts)   # 각 단어와 각 단어의 빈도를 bow에 저장\n",
        "\n",
        "  print('Bag of words:', bow)    # bow 출력\n",
        "  print('단어장(vocabulary)의 크기:', len(tokenizer.word_counts))    # 중복을 제거한 단어들의 개수"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djfq2vTEs_E5",
        "outputId": "ae1e3f6d-f480-4995-c52a-ca6a8c7be5c6"
      },
      "source": [
        "print_bow(sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of words: {'john': 1, 'likes': 3, 'to': 2, 'watch': 2, 'movies': 2, 'mary': 2, 'too': 1, 'also': 1, 'football': 1, 'games': 1}\n",
            "단어장(vocabulary)의 크기: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQTA-n8wtZCb"
      },
      "source": [
        "## scikit-learn CountVectorizer를 활용한 BoW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEICeh6etDAj",
        "outputId": "106fee4f-0a0f-487b-8145-0d60364cadb5"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "\n",
        "print('Bag of words:', vector.fit_transform(sentence).toarray())    # 코퍼스로부터 각 단어의 빈도수를 기록\n",
        "print('각 단어의 인덱스:', vector.vocabulary_)    # 각 단어의 인덱스가 어떻게 부여되는지를 보여줌"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bag of words: [[1 1 1 1 3 2 2 2 1 2]]\n",
            "각 단어의 인덱스: {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wC4xjjuj0b"
      },
      "source": [
        "## 불용어를 제거한 BoW 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn8FP1a7uzub"
      },
      "source": [
        "### 사용자가 직접 정의한 불용어 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msmEkn-CuChE",
        "outputId": "b6df50ff-5675-4b10-e81b-a884ac4eb807"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vect = CountVectorizer(stop_words=['the', 'a', 'an' ,'is', 'not'])\n",
        "print(vect.fit_transform(text).toarray())\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1]]\n",
            "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW9gBScGviQT"
      },
      "source": [
        "### CountVectorizer에서 제공하는 자체 불용어 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl4h8UXyvPsP",
        "outputId": "1186ddf2-f6d2-4847-ea69-c24185acea93"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "print(vect.fit_transform(text).toarray())\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1]]\n",
            "{'family': 0, 'important': 1, 'thing': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGQepwRpv50x"
      },
      "source": [
        "### NLTK에서 지원하는 불용어 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AujhCytwXzU",
        "outputId": "eed2f714-1fda-4fbe-f0a9-3e12b2bdbd08"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQmQJp0tvwoO",
        "outputId": "69dc25d6-93ee-4963-99db-692da6e36322"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "sw = stopwords.words('english')\n",
        "vect = CountVectorizer(stop_words=sw)\n",
        "print(vect.fit_transform(text).toarray())\n",
        "print(vect.vocabulary_)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1]]\n",
            "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvuQck5Uy4rS"
      },
      "source": [
        "## DTW(Document-Term Matrix)\n",
        "다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것  \n",
        "다수의 문서에 대해서 BoW를 하나의 행렬로 표현하고 부르는 용어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJZL6kyR0TR7"
      },
      "source": [
        "문서 1: I like dog  \n",
        "문서 2: I like cat  \n",
        "문서 3: I like cat I like cat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "HIyYm3LxwRUE",
        "outputId": "a18b2857-126d-43e3-e2b5-87c0006da402"
      },
      "source": [
        "import pandas as pd\n",
        "content = [[0, 1, 1, 1], [1, 0, 1, 1], [2, 0, 2, 2]]\n",
        "df = pd.DataFrame(content)\n",
        "df.index = ['(문서1) I like dog','(문서2) I like cat','(문서3) I like cat I like cat']\n",
        "df.columns = ['cat','dog','I','like']\n",
        "df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>I</th>\n",
              "      <th>like</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(문서1) I like dog</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(문서2) I like cat</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(문서3) I like cat I like cat</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             cat  dog  I  like\n",
              "(문서1) I like dog               0    1  1     1\n",
              "(문서2) I like cat               1    0  1     1\n",
              "(문서3) I like cat I like cat    2    0  2     2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9veKcF080wY2"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm    # norm: 벡터 크기 또는 길이를 측정하는 방법\n",
        "\n",
        "doc1 = np.array([0,1,1,1])\n",
        "doc2 = np.array([1,0,1,1])\n",
        "doc3 = np.array([2,0,2,2])\n",
        "\n",
        "def cos_sim(A,B):\n",
        "  return dot(A,B)/(norm(A)*norm(B))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7PP-2wc12LF",
        "outputId": "8e82377e-7dfd-4937-efaf-2db3328a9977"
      },
      "source": [
        "print(cos_sim(doc1, doc2))\n",
        "print(cos_sim(doc1, doc3))\n",
        "print(cos_sim(doc2, doc3))    # 코사인 유사도는 0~1사이의 값을 가지며, 1에 가까울수록 유사도가 높다고 판단"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6666666666666667\n",
            "0.6666666666666667\n",
            "1.0000000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-VTl9CV2R0c"
      },
      "source": [
        "### scikit-learn CountVectorizer 활용한 DTM 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VFAZsGS19Eb",
        "outputId": "817c5c5c-e9e8-472b-ee3c-39bca6d28ef6"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "          'John likes to watch movies',\n",
        "          'Mary likes movies too',\n",
        "          'Mary also likes to watch football games',\n",
        "]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray())    # 각 인덱스에 해당하는 단어가 몇 개 들어 있는지\n",
        "print(vector.vocabulary_)    # 단어 사전"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 1 0 1 1 0 1]\n",
            " [0 0 0 0 1 1 1 0 1 0]\n",
            " [1 1 1 0 1 1 0 1 0 1]]\n",
            "{'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmf1gr6y3KEe"
      },
      "source": [
        "한계점\n",
        "1. 희소표현(sparse representation)이다.\n",
        "2. 단순 빈도수 기반 접근이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFDdW7NY3kVF"
      },
      "source": [
        "## TF-IDF(Term Frequency - Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKcc45HP6Bib"
      },
      "source": [
        "모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하고, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단하는 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9FJdtAS2uue"
      },
      "source": [
        "from math import log\n",
        "import pandas as pd\n",
        "\n",
        "docs = [\n",
        "        'John likes to watch movies and Mary likes movies too',\n",
        "        'James likes to watch TV',\n",
        "        'Mary also likes to watch football gaes',\n",
        "]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTMVN-U6cWE",
        "outputId": "10d2850c-c8b0-4f20-b26f-255bcd228dab"
      },
      "source": [
        "vocab = list(set(w for doc in docs for w in doc.split()))\n",
        "vocab.sort()\n",
        "print('단어장의 크기:', len(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어장의 크기: 13\n",
            "['James', 'John', 'Mary', 'TV', 'also', 'and', 'football', 'gaes', 'likes', 'movies', 'to', 'too', 'watch']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96YJnrwc6osS",
        "outputId": "ec6ac06f-122a-4a00-9900-4a9912f3c342"
      },
      "source": [
        "N = len(docs)\n",
        "N"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFT-H8w-60SL"
      },
      "source": [
        "def tf(t, d):\n",
        "  return d.count(t)\n",
        "\n",
        "def idf(t):\n",
        "  df = 0\n",
        "  for doc in docs:\n",
        "    df += t in doc\n",
        "  return log(N/(df + 1)) + 1\n",
        "\n",
        "def tfidf(t, d):\n",
        "  return tf(t, d) * idf(t)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "bunv9RDK7Q3r",
        "outputId": "4e17407f-a9a6-49df-a354-745e79efff61"
      },
      "source": [
        "result = []\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d = docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t = vocab[j]\n",
        "\n",
        "    result[-1].append(tf(t,d))\n",
        "\n",
        "tf_ = pd.DataFrame(result, columns=vocab)\n",
        "tf_"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>James</th>\n",
              "      <th>John</th>\n",
              "      <th>Mary</th>\n",
              "      <th>TV</th>\n",
              "      <th>also</th>\n",
              "      <th>and</th>\n",
              "      <th>football</th>\n",
              "      <th>gaes</th>\n",
              "      <th>likes</th>\n",
              "      <th>movies</th>\n",
              "      <th>to</th>\n",
              "      <th>too</th>\n",
              "      <th>watch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   James  John  Mary  TV  also  and  ...  gaes  likes  movies  to  too  watch\n",
              "0      0     1     1   0     0    1  ...     0      2       2   2    1      1\n",
              "1      1     0     0   1     0    0  ...     0      1       0   1    0      1\n",
              "2      0     0     1   0     1    0  ...     1      1       0   1    0      1\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "HKVicJTv7nHD",
        "outputId": "1c453ce3-7bad-4217-800f-cecc89328bb3"
      },
      "source": [
        "result = []\n",
        "for j in range(len(vocab)):\n",
        "  t = vocab[j]\n",
        "  result.append(idf(t))\n",
        "\n",
        "idf_ = pd.DataFrame(result, index=vocab, columns=['IDF'])\n",
        "idf_"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>James</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>John</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mary</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TV</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>also</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>football</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gaes</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>likes</th>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movies</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>too</th>\n",
              "      <td>1.405465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>watch</th>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               IDF\n",
              "James     1.405465\n",
              "John      1.405465\n",
              "Mary      1.000000\n",
              "TV        1.405465\n",
              "also      1.405465\n",
              "and       1.405465\n",
              "football  1.405465\n",
              "gaes      1.405465\n",
              "likes     0.712318\n",
              "movies    1.405465\n",
              "to        0.712318\n",
              "too       1.405465\n",
              "watch     0.712318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "dRIsP5g374Hl",
        "outputId": "d1a2326b-c943-4a4e-9c5b-a21fe1e4f049"
      },
      "source": [
        "result = []\n",
        "for i in range(N):\n",
        "  result.append([])\n",
        "  d = docs[i]\n",
        "  for j in range(len(vocab)):\n",
        "    t = vocab[j]\n",
        "\n",
        "    result[-1].append(tfidf(t, d))\n",
        "\n",
        "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
        "tfidf_"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>James</th>\n",
              "      <th>John</th>\n",
              "      <th>Mary</th>\n",
              "      <th>TV</th>\n",
              "      <th>also</th>\n",
              "      <th>and</th>\n",
              "      <th>football</th>\n",
              "      <th>gaes</th>\n",
              "      <th>likes</th>\n",
              "      <th>movies</th>\n",
              "      <th>to</th>\n",
              "      <th>too</th>\n",
              "      <th>watch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.424636</td>\n",
              "      <td>2.81093</td>\n",
              "      <td>1.424636</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712318</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.712318</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>1.405465</td>\n",
              "      <td>0.712318</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.712318</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.712318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      James      John  Mary        TV  ...   movies        to       too     watch\n",
              "0  0.000000  1.405465   1.0  0.000000  ...  2.81093  1.424636  1.405465  0.712318\n",
              "1  1.405465  0.000000   0.0  1.405465  ...  0.00000  0.712318  0.000000  0.712318\n",
              "2  0.000000  0.000000   1.0  0.000000  ...  0.00000  0.712318  0.000000  0.712318\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJJg9ln8fKm"
      },
      "source": [
        "0: 'John likes to watch movies and Mary likes movies too'  \n",
        "1: 'James likes to watch TV'  \n",
        "2: 'Mary also likes to watch football gaes'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmE1pMGc9FIY"
      },
      "source": [
        "### scikit-learn 활용한 TF-IDF 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOa7_1WP8Qg_",
        "outputId": "01a45695-c2a1-4997-d821-5831757ddeae"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "          'you know I want your love',\n",
        "          'I like you',\n",
        "          'what should I do'\n",
        "]\n",
        "\n",
        "vector = CountVectorizer()\n",
        "print(vector.fit_transform(corpus).toarray())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 0 1 0 1 1]\n",
            " [0 0 1 0 0 0 0 1 0]\n",
            " [1 0 0 0 1 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X9sjFv49iYE",
        "outputId": "262d90d8-90f1-478d-a64c-b3dc990906cc"
      },
      "source": [
        "print(vector.vocabulary_)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kthZDRc59mot"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "          'you know I want your love',\n",
        "          'I like you',\n",
        "          'what should I do'\n",
        "]\n",
        "\n",
        "tfidfv = TfidfVectorizer().fit(corpus)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRVymOJk-CqG",
        "outputId": "e2ca03ec-721f-43da-fa3f-987649913d04"
      },
      "source": [
        "print(tfidfv.transform(corpus).toarray())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
            "  0.         0.35543247 0.46735098]\n",
            " [0.         0.         0.79596054 0.         0.         0.\n",
            "  0.         0.60534851 0.        ]\n",
            " [0.57735027 0.         0.         0.         0.57735027 0.\n",
            "  0.57735027 0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkl5Ocjv-Fw4",
        "outputId": "8d634255-5db5-4d7e-ba3f-111a3d681852"
      },
      "source": [
        "print(tfidfv.vocabulary_)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX1m7CMZCHm0"
      },
      "source": [
        "## abc뉴스데이터로 TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHitU20H-IHm"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X46NlGIuCZCN",
        "outputId": "c11785d2-3c24-4532-85bb-bfdbc6749d8b"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWryMex8CeLt",
        "outputId": "fdc4b84c-2c91-43dc-9025-5e9ac2445440"
      },
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\",\n",
        "                           filename=\"/content/abcnews-data-text.csv\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/abcnews-data-text.csv', <http.client.HTTPMessage at 0x7ff13b174290>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DplUdzCCvoB"
      },
      "source": [
        "data = pd.read_csv('/content/abcnews-data-text.csv', error_bad_lines=False)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Ta3QmV7MDgHU",
        "outputId": "309601dd-8446-47a4-f789-fa37b322aec6"
      },
      "source": [
        "data"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20030219</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20030219</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20030219</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082163</th>\n",
              "      <td>20170630</td>\n",
              "      <td>when is it ok to compliment a womans smile a g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082164</th>\n",
              "      <td>20170630</td>\n",
              "      <td>white house defends trumps tweet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082165</th>\n",
              "      <td>20170630</td>\n",
              "      <td>winter closes in on tasmania as snow ice falls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082166</th>\n",
              "      <td>20170630</td>\n",
              "      <td>womens world cup australia wins despite atapat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082167</th>\n",
              "      <td>20170630</td>\n",
              "      <td>youtube stunt death foreshadowed by tweet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1082168 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         publish_date                                      headline_text\n",
              "0            20030219  aba decides against community broadcasting lic...\n",
              "1            20030219     act fire witnesses must be aware of defamation\n",
              "2            20030219     a g calls for infrastructure protection summit\n",
              "3            20030219           air nz staff in aust strike for pay rise\n",
              "4            20030219      air nz strike to affect australian travellers\n",
              "...               ...                                                ...\n",
              "1082163      20170630  when is it ok to compliment a womans smile a g...\n",
              "1082164      20170630                   white house defends trumps tweet\n",
              "1082165      20170630     winter closes in on tasmania as snow ice falls\n",
              "1082166      20170630  womens world cup australia wins despite atapat...\n",
              "1082167      20170630          youtube stunt death foreshadowed by tweet\n",
              "\n",
              "[1082168 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nX5vhSJRDikb",
        "outputId": "a7047bbc-e7c0-43c7-e972-06fdb39da864"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20030219</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20030219</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20030219</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20030219</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   publish_date                                      headline_text\n",
              "0      20030219  aba decides against community broadcasting lic...\n",
              "1      20030219     act fire witnesses must be aware of defamation\n",
              "2      20030219     a g calls for infrastructure protection summit\n",
              "3      20030219           air nz staff in aust strike for pay rise\n",
              "4      20030219      air nz strike to affect australian travellers"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcoz9ZDWDr1V"
      },
      "source": [
        "text = data[['headline_text']]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJIf7S__D1sY",
        "outputId": "e4e4772c-2756-47b3-f19b-add463916874"
      },
      "source": [
        "text.nunique()    # 고유값 개수"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "headline_text    1054983\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPWZDvFeD6Us",
        "outputId": "ae6ca492-a1d2-4276-9021-bab81130ac8b"
      },
      "source": [
        "text.drop_duplicates(inplace=True)\n",
        "text = text.reset_index(drop=True)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u_dYyy9ES_g",
        "outputId": "d9e460bc-279e-4826-fd85-25a9fcd1264a"
      },
      "source": [
        "print(len(text))    # 중복이 제거됨"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1054983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vniTG_XxEbec"
      },
      "source": [
        "## 데이터를 정제 및 정규화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93vefGrOEVVl"
      },
      "source": [
        "# NLTK 토크나이저를 이용해 토큰화\n",
        "text['headline_text'] = text.apply(lambda row:nltk.word_tokenize(row['headline_text']), axis=1)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sICr8BpREsKI"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop_words)])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "u5r2ny84Fci2",
        "outputId": "6b11f238-92a7-436d-d17a-46953353e9e4"
      },
      "source": [
        "text.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[aba, decides, community, broadcasting, licence]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[act, fire, witnesses, must, aware, defamation]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[g, calls, infrastructure, protection, summit]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[air, nz, staff, aust, strike, pay, rise]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[air, nz, strike, affect, australian, travellers]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       headline_text\n",
              "0   [aba, decides, community, broadcasting, licence]\n",
              "1    [act, fire, witnesses, must, aware, defamation]\n",
              "2     [g, calls, infrastructure, protection, summit]\n",
              "3          [air, nz, staff, aust, strike, pay, rise]\n",
              "4  [air, nz, strike, affect, australian, travellers]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGO6CtgHFk_A"
      },
      "source": [
        "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4Eb-3jLF7Bd"
      },
      "source": [
        "# 길이가 1~2인 것 제거\n",
        "text = text['headline_text'].apply(lambda x:[word for word in x if len(word)>2])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8d2fQfUGPIf",
        "outputId": "1ada2ea2-5d21-4e85-edb3-853f6ac985d8"
      },
      "source": [
        "print(text[:5])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0     [aba, decide, community, broadcast, licence]\n",
            "1    [act, fire, witness, must, aware, defamation]\n",
            "2       [call, infrastructure, protection, summit]\n",
            "3            [air, staff, aust, strike, pay, rise]\n",
            "4    [air, strike, affect, australian, travellers]\n",
            "Name: headline_text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lknGTx1lGR9-"
      },
      "source": [
        "detokenized_doc = []\n",
        "for i in range(len(text)):\n",
        "  t = ' '.join(text[i])\n",
        "  detokenized_doc.append(t)\n",
        "\n",
        "train_data = detokenized_doc"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkZ2CteVGj8d",
        "outputId": "00e13dbc-9246-4d34-d703-b5c020763fb7"
      },
      "source": [
        "train_data[:5]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aba decide community broadcast licence',\n",
              " 'act fire witness must aware defamation',\n",
              " 'call infrastructure protection summit',\n",
              " 'air staff aust strike pay rise',\n",
              " 'air strike affect australian travellers']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59SZXftoGrKq"
      },
      "source": [
        "c_vectorizer = CountVectorizer(stop_words='english', max_features=5000)    # 상위 5000개만 사용\n",
        "document_term_matrix = c_vectorizer.fit_transform(train_data)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9pMYapJG-Kg",
        "outputId": "23313e06-b220-48dc-c7f7-72d7eb41046b"
      },
      "source": [
        "print('행렬의 크기:', document_term_matrix.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "행렬의 크기: (1054983, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkaBaN_SHJ-e"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tf_idf_matrix = tfidf_vectorizer.fit_transform(train_data)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiEEu8i8HRJU",
        "outputId": "f4b5e2d7-b9e7-41e5-fb89-b989d8d7380f"
      },
      "source": [
        "print('행렬의 크기:', tf_idf_matrix.shape)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "행렬의 크기: (1054983, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}